{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Obtain_Representations.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1uXRlA-W3cGZh8hWt6_dslK0uqbAoa_35","authorship_tag":"ABX9TyNWqfCoLmGG9fenvoM9AeV9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guR7VlGfXA7v","executionInfo":{"status":"ok","timestamp":1621436362685,"user_tz":-600,"elapsed":931051,"user":{"displayName":"Mohil Tanti","photoUrl":"","userId":"06002771175330970661"}},"outputId":"92039751-33c7-4b8f-fd10-f226652593cb"},"source":["from random import randrange\n","import os\n","import csv\n","import pickle\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","\n","from keras.layers import Input\n","from keras.models import load_model, Model\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","\n","\n","LOSS_FUNCTION = 'binary_crossentropy'\n","OPTIMIZER = 'sgd'\n","\n","project_name = 'ffmpeg'\n","\n","MAX_LEN = 1000 \n","EMBEDDING_DIM = 100 \n","\n","\n","#--------------------------------------------------------#\n","# 2. Load the data which will be the inputs for obtaining the representations.\n","\n","def getData(file_path):\n","\t\n","\tdf = pd.read_csv(file_path, sep=',', encoding='latin1', low_memory=False, header=None) \n","\tdf_list = df.values.tolist()\n","\t\n","\ttemp = []\n","\tid_list = []\n","\tfor i in df_list:\n","\t\t# Get rid of 'NaN' values.\n","\t\ti = [x for x in i if str(x) != 'nan']\n","\t\ti = [str(x) for x in i]\n","\t\tif len(i) > 3:\n","\t\t\tid_element = i[0] \n","\t\t\ttemp.append(i[1:]) \n","\t\t\tid_list.append(id_element)\n","\t\n","\treturn temp, id_list\n","\n","def ProcessList_1(list_to_process):\n","\ttoken_list = []\n","\tfor sub_list_to_process in list_to_process:\n","\t\tsub_token_list = []\n","\t\tfor each_word in sub_list_to_process:\n","\t\t\tsub_word = each_word.split()\n","\t\t\tfor element in sub_word:\n","\t\t\t\tsub_token_list.append(element)\n","\t\ttoken_list.append(sub_token_list)\n","\treturn token_list\n","\n","def LoadSavedData(path):\n","\twith open(path, 'rb') as f:\n","\t\tloaded_data = pickle.load(f)\n","\treturn loaded_data\n","\n","def processList(list_to_process):\n","\ttemp = []\n","\tfor i in list_to_process:\n","\t\ti = [x for x in i if str(x) != ';'] \n","\t\ttemp.append(i)\n","\t\n","\treturn temp\n","\n","def storeOuput(arr, path):\n","\twith open(path, 'w') as myfile:\n","\t\twr = csv.writer(myfile)\n","\t\twr.writerow(arr)\n","\t\t\n","def ListToCSV(list_to_csv, path):\n","\tdf = pd.DataFrame(list_to_csv)\n","\tdf.to_csv(path, index=False)\n","\n","def GenerateLabels(input_arr):\n","\ttemp_arr = []\n","\tfor func_id in input_arr:\n","\t\ttemp_sub_arr = []\n","\t\tif \"cve\" in func_id or \"CVE\" in func_id:\n","\t\t\ttemp_sub_arr.append(1)\n","\t\telse:\n","\t\t\ttemp_sub_arr.append(0)\n","\t\ttemp_arr.append(temp_sub_arr)\n","\treturn np.asarray(temp_arr)\n","#\n","cwe399_file_list_label = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE399/CGD/LBL_CWE399.pkl')\n","\n","cwe119_file_list_label = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE119/CGD/LBL_CWE119.pkl')\n","\n","\n","cwe399_file_list = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE399/CGD/AST_CWE399.pkl')\n","\n","\n","cwe119_file_list = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE119/CGD/AST_CWE119.pkl')\n","\n","\n","\n","\n","cwe399_list_label = (np.asarray(cwe399_file_list_label).flatten()).tolist()\n","\n","cwe119_list_label = (np.asarray(cwe119_file_list_label).flatten()).tolist()\n","\n","total_399_list = processList(cwe399_file_list)\n","\n","total_119_list = processList(cwe119_file_list)\n","\n","\n","vul_399_list = []\n","vul_399_list_id = []\n","nvul_399_list = []\n","nvul_399_list_id = []\n","\n","vul_119_list = []\n","vul_119_list_id = []\n","nvul_119_list = []\n","nvul_119_list_id = []\n","\n","\n","#--------------------------------------------------------#\n","# 3. Load the saved model and compile it.\n","\n","model = load_model(\"/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/models/model.h5\")\n","\n","model.compile(loss= LOSS_FUNCTION,\n","\t\t\t  optimizer=OPTIMIZER,\n","\t\t\t  metrics=['accuracy'])\n","\n","print (\"The model has been loaded: \")\n","print (model.summary())\n","\n","\n","#------------------------------------#\n","# 4. Load pre-trained word2vec and tokens\n","\n","print (\"Applying tokenization....\")\n","\n","def JoinSubLists(list_to_join):\n","\tnew_list = []\n","\t\n","\tfor sub_list_token in list_to_join:\n","\t\tnew_line = ','.join(sub_list_token)\n","\t\tnew_list.append(new_line)\n","\treturn new_list\n","\n","total_399_list_new = JoinSubLists(total_399_list)\n","total_119_list_new = JoinSubLists(total_119_list)\n","\n","\n","tokenizer = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/tokenizer.pickle')\n","total_399_sequences = tokenizer.texts_to_sequences(total_399_list_new)\n","total_119_sequences = tokenizer.texts_to_sequences(total_119_list_new)\n","\n","\n","\n","word_index = tokenizer.word_index\n","print ('Found %s unique tokens.' % len(word_index))\n","\n","print (\"The length of tokenized 399 sequences: \" + str(len(total_399_sequences)))\n","print (\"The length of tokenized 119 sequences: \" + str(len(total_119_sequences)))\n","\n","print (\"Loading trained word2vec model...\")\n","\n","# Load the pre-trained embeddings.\n","w2v_model_path = '/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/w2v_model.txt'\n","w2v_model = open(w2v_model_path, encoding=\"latin1\")\n","\n","print (\"----------------------------------------\")\n","print (\"The trained word2vec model: \")\n","print (w2v_model)\n","\n","#------------------------------------#\n","# 3. Do the paddings.\n","print (\"max_len \", MAX_LEN)\n","print('Pad sequences (samples x time)')\n","\n","total_399_sequences_pad = pad_sequences(total_399_sequences, maxlen = MAX_LEN, padding ='post')\n","total_119_sequences_pad = pad_sequences(total_119_sequences, maxlen = MAX_LEN, padding ='post')\n","print (total_399_sequences_pad.shape)\n","print (total_119_sequences_pad.shape)\n","\n","\n","# Acquire the embeddings.\n","def ObtainRepresentations(input_sequences, layer_number, model):\n","\tlayered_model = Model(inputs = model.input, outputs=model.layers[layer_number].output)\n","\trepresentations = layered_model.predict(input_sequences)\n","\treturn representations\n","\n","# There are 9 layers in total. So we use the fourth last layer\n","total_399_repre_nist = ObtainRepresentations(total_399_sequences_pad, 5, model) \n","total_119_repre_nist = ObtainRepresentations(total_119_sequences_pad, 5, model) \n","\n","\n","\n","np.savetxt('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/total_399_nist_rep_128.csv', total_399_repre_nist, delimiter=\",\")\n","np.savetxt('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/total_399_nist_label.csv', cwe399_list_label, delimiter=\",\")\n","np.savetxt('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/total_119_nist_rep_128.csv', total_119_repre_nist, delimiter=\",\")\n","np.savetxt('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/total_119_nist_label.csv', cwe119_list_label, delimiter=\",\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has been loaded: \n","Model: \"BiLSTM_network\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 1000)]            0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 1000, 100)         1821300   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 1000, 128)         84480     \n","_________________________________________________________________\n","dropout (Dropout)            (None, 1000, 128)         0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 1000, 128)         98816     \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 128)               0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                8256      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                2080      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 2,014,965\n","Trainable params: 193,665\n","Non-trainable params: 1,821,300\n","_________________________________________________________________\n","None\n","Applying tokenization....\n","Found 18212 unique tokens.\n","The length of tokenized 399 sequences: 21885\n","The length of tokenized 119 sequences: 39753\n","Loading trained word2vec model...\n","----------------------------------------\n","The trained word2vec model: \n","<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/w2v_model.txt' mode='r' encoding='latin1'>\n","max_len  1000\n","Pad sequences (samples x time)\n","(21885, 1000)\n","(39753, 1000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8J2IaGvf-Xev"},"source":[""],"execution_count":null,"outputs":[]}]}