{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_Word2Vec.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1onbvTD3mfJ5ro8MaS6JiK7qiosY1581f","authorship_tag":"ABX9TyPwkLd0xivdHA8CDLIbFaD2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vdVhtc3rsxoL","executionInfo":{"status":"ok","timestamp":1621392568563,"user_tz":-600,"elapsed":37372,"user":{"displayName":"Mohil Tanti","photoUrl":"","userId":"06002771175330970661"}},"outputId":"a1c693db-2bd8-4b31-dbb0-4e21ce42a4c9"},"source":["import time\n","import pickle\n","import os\n","import pandas as pd\n","\n","from gensim.models import Word2Vec\n","from keras.preprocessing.text import Tokenizer\n","\n","script_start_time = time.time()\n","\n","print (\"Script starts at: \" + str(script_start_time))\n","\n","#--------------------------------------------------------#\n","# 1. Load data from .csv files. \n","\n","def getData(filePath):\n","    df = pd.read_csv(filePath, sep=\",\", low_memory=False)\n","    df_list = df.values.tolist()\n","    temp = []\n","    for i in df_list:\n","        i = [x for x in i if str(x) != 'nan']\n","        temp.append(i)\n","    \n","    return temp\n","\n","def ProcessList(list_to_process):\n","    token_list = []\n","    for sub_list_to_process in list_to_process:\n","        sub_token_list = []\n","        for each_word in sub_list_to_process:\n","            sub_word = each_word.split()\n","            for element in sub_word:\n","                sub_token_list.append(element)\n","        token_list.append(sub_token_list)\n","    return token_list\n","\n","\n","def LoadSavedData(path):\n","    with open(path, 'rb') as f:\n","        loaded_data = pickle.load(f)\n","    return loaded_data\n","\n","\n","cwe399_file_list = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE399/CGD/AST_CWE399.pkl')\n","\n","\n","cwe119_file_list = LoadSavedData('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/CWE119/CGD/AST_CWE119.pkl')\n","\n","\n","\n","total_list =  cwe119_file_list + cwe119_file_list\n","\n","\n","train_token_list = ProcessList(total_list)\n","\n","print (\"The length of training set is : \" + str(len(train_token_list)))\n","\n","#--------------------------------------------------------#\n","# 2. Tokenization: convert the loaded text (the nodes of ASTs) to tokens.\n","\n","new_total_token_list = []\n","\n","for sub_list_token in train_token_list:\n","    new_line = ','.join(sub_list_token)\n","    new_total_token_list.append(new_line)\n","\n","tokenizer = Tokenizer(num_words=None, filters=',', lower=False, char_level=False)\n","tokenizer.fit_on_texts(new_total_token_list)\n","\n","# Save the tokenizer.\n","with open('/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# ----------------------------------------------------- #\n","# 3. Train a Vocabulary with Word2Vec -- using the function provided by gensim\n","\n","w2vModel = Word2Vec(train_token_list, workers = 12, size=100)\n","\n","print (\"----------------------------------------\")\n","print (\"The trained word2vec model: \")\n","print (w2vModel)\n","\n","w2vModel.wv.save_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/DSCP/Dataset/w2v_model.txt\", binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Script starts at: 1621392534.815187\n","The length of training set is : 79506\n","----------------------------------------\n","The trained word2vec model: \n","Word2Vec(vocab=9143, size=100, alpha=0.025)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e07xTjW7t4eQ"},"source":[""],"execution_count":null,"outputs":[]}]}